--------------------------------------------------------------------------------
title: **"Recognition of quality of exercises based on Weight Lifting Exercises data"**
author: "AShulga"
date: "August 17, 2015"
--------------------------------------------------------------------------------


### Synopsys
In this research we try to predict the quility of weigth lifting exercises perforemed by 6 participants. We base our analysis on Weight Lifting Exercises dataset (source: http://groupware.les.inf.puc-rio.br/har). There are 6 types of "behavour" classes which we try to predict (Class A corresponds to the specified execution of the exercise, while the other 4 classes correspond to common mistakes) based on the data from accelerometers on the belt, forearm, arm, and dumbell of the participants. To come up with a solid estimate of the out-of-sample error we developed an ensamble model with bagging method. There are three models in a bag, each tuned with 10-fold cross-validation technic: Boosted trees model, SVM and Random Forests. Final model is based on majority vote of those three. Out-of-sample error is estimated for every of the three models, Random Forest demonstrates the most accurate result (Error is 0.4%).     


``` {r, results='hide', warning=FALSE}
## General 
Sys.setlocale("LC_ALL", "english")
require(ggplot2); require (caret) 
require(randomForest); require(kernlab); require(gbm)
set.seed(12344321) ## for replicability 
```

```{r}
## Load the data
if (!exists("trainData") || !exists("testData")) {
        if (length(grep("pml-training.csv", dir())) <= 0) {
                download.file(url = "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", destfile = "pml-training.csv")
        } 
        if (length(grep("pml-testing.csv", dir())) <= 0) {
                download.file(url = "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv", destfile = "pml-testing.csv")
        }  
        trainData <- read.csv("pml-training.csv", header = TRUE)
        testData <- read.csv("pml-testing.csv", header = TRUE)
}

```


### Data preparation and analysis
The dataset consists of 159 predictors representing data generated by 4 sensors across different parameters. Standard range of statistics is calculated for sensor data. Our strategy is to get rid of predictors that contain little relevant information (have little varience or too many NAs in the data) or are highly correlated. We would also ignore timestamps in our analysis.   

```{r}
## find and remove Near Zero Varience predictors
trainDS <- trainData[,-nearZeroVar(trainData)]

## remove timestamps and other data that contains poor signal (too many NAs)
trainDS <- trainDS[,-c(1,3:6)]
trainDS <- trainDS[,-which(apply(is.na(trainDS),2,function(x) {sum(x)/length(x)})>0.95)]

## find and remove predictors which are highly correlated (90% correlation)
highCorr <- findCorrelation(cor(trainDS[,-c(1,ncol(trainDS))]), 0.90)
trainDS <- trainDS[,-(highCorr+1)]

ncol(trainDS)-1 ## show remaining number of predictors

## The following code is for additional validation test 
##train_index <- createDataPartition(trainDS$classe, p = 0.90, list = FALSE)
##trainDS <- trainDS[train_index,]
##testDS <- trainDS[-train_index,]
```

As we can see only 46 predictors are left for our analysis. The figure below demonstrates that the correlation betwean remaining predictors is moderate.
```{r,fig.height=4,fig.width=6}
heatmap(cor(trainDS[,-c(1,ncol(trainDS))]), main = "Predictors correlations map")
```
 

### Model development
Our strategy is to build 3 types of models and then create an ensamble that will produce final predictions via majority vote method. Each model will be tuned based on 10-folds cross-validation.

Our first model is Boosted trees model tuned with predictors interaction up to 6 and number of trees up to 200.
```{r, cache=TRUE}
gbmGrid <- expand.grid(interaction.depth = c(1,3)*2, n.trees = c(1, 4, 8)*25, shrinkage = .1, n.minobsinnode = 10)
bootControl = trainControl(method = "cv", number = 10)
set.seed(135) ## to compare iterations
fitGBM <- train(classe ~ ., data = trainDS, method="gbm", trControl = bootControl, verbose = FALSE, tuneGrid = gbmGrid)
fitGBM
##plot(fitGBM)
##confusionMatrix(predict(fitGBM, testDS), testDS$classe)
```

Our second model is Support Vector Machines model used on PCA preprocessed data.
```{r, cache=TRUE}
bootControl = trainControl(method = "cv", number = 10)
set.seed(135) ## to compare iterations
fitSVM <- train(classe ~ ., data = trainDS, method="svmRadial", preProcess = ("pca"), PCAthresh = 0.9, tuneLength = 3, trControl = bootControl, scaled = FALSE)
fitSVM
##confusionMatrix(predict(fitSVM, testDS), testDS$classe)
```

Our third model is Random Forests model with number of trees up to 100 and tuned over randomly selected predictors.
```{r, cache=TRUE}
rfGrid <- expand.grid(mtry = c(1,3,6,9,12))
bootControl = trainControl(method = "cv", number = 10)
set.seed(135) ## to compare iterations
fitRF <- train(classe ~ ., data = trainDS, method="rf", trControl = bootControl, ntree = 100, tuneGrid = rfGrid)
fitRF
##confusionMatrix(predict(fitRF, testDS), testDS$classe)
```

Below is out-of-sample Accuracy estimates for each of the model developed. Accuracy is measured across same cross-validation data folds for each of the models so it can be compared between the models. 
```{r,fig.height=4,fig.width=6}
resamps <- resamples(list(RF = fitRF, GBM = fitGBM, SVM = fitSVM))
trellis.par.set(caretTheme())
dotplot(resamps, metric = "Accuracy", main = "Accuracy comparison across the models")
```

As we can see all three models have comparably high Accuracy (out-of-sample estimate is in the range of 93.5-99.6%) with Random Forest demonstrating the highest result (99.6%). 

### Development of ensemble model
Our ensamble model will be based on bagging method with each of the 3 models voting for prediction result. Final prediction will be based on majority vote method.  
```{r}
models <- list(GBM = fitGBM, SVM = fitSVM, RF = fitRF)
result <- predict(models, testData)
result <- as.data.frame(result)
t_vote <- numeric(5)
votes <- apply(result,1,function(x) {
        for (i in 1:5) t_vote[i] <- sum((x == LETTERS[i])*1)
        LETTERS[which(t_vote == max(t_vote))]
})
##confusionMatrix(as.factor(votes),testDS$classe) ## used for validation 
votes
```

Above is a printout of test set predictions to be submitted to Coursera.

### Creation of files with answers for submission
According to our assignment below is the code for creation of files with the test dataset predictions to be submitted to Coursera.
```{r, eval=TRUE}
if (!dir.exists("test_answers")) dir.create("test_answers")
setwd("test_answers")
for(i in 1:length(votes)){
    filename = paste0("problem_id_",i,".txt")
    write.table(votes[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
}
setwd("..")
```

